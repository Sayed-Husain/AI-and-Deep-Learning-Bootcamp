<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML5.js Face Recognition Demo</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }

        h1 {
            text-align: center;
        }

        #face-recognition {
            text-align: center;
        }

        video-container {
            border: 1px solid #ddd;
            border-radius: 8px;
        }
    </style>
</head>
<body>
    <h1>ML5.js Face Recognition Demo</h1>
    <div id="face-recognition">
        <h2>Face Recognition with Landmark Detection</h2>
        <p>Allow access to your camera to see real-time face detection and landmarks.</p>
        <div id="video-container">
        </div>
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.0/p5.js"></script>
    <script src="https://unpkg.com/ml5@0.12.2/dist/ml5.min.js"></script>
    <script>
    let faceapi;
      let video;
      let detections;

      // by default all options are set to true
      const detection_options = {
        withLandmarks: true,
        withDescriptors: false,
      }


      function setup() {
        createCanvas(360, 270).parent("#video-container");

        // load up your video
        video = createCapture(VIDEO);
        video.size(width, height);
        video.parent("#video-container");
        // video.hide(); // Hide the video element, and just show the canvas
        faceapi = ml5.faceApi(video, detection_options, modelReady)
        textAlign(RIGHT);
      }

      function modelReady() {
        console.log('ready!')
        console.log(faceapi)
        faceapi.detect(gotResults)

      }

      function gotResults(err, result) {
        if (err) {
            console.log(err)
            return
        }
        // console.log(result)
        detections = result;

        // background(220);
        background(255);
        image(video, 0,0, width, height)
        if (detections) {
            if (detections.length > 0) {
                // console.log(detections)
                drawBox(detections)
                drawLandmarks(detections)
            }

        }
        faceapi.detect(gotResults)
      }

      function drawBox(detections){
        for(let i = 0; i < detections.length; i++){
            const alignedRect = detections[i].alignedRect;
            const x = alignedRect._box._x
            const y = alignedRect._box._y
            const boxWidth = alignedRect._box._width
            const boxHeight  = alignedRect._box._height

            noFill();
            stroke(161, 95, 251);
            strokeWeight(2);
            rect(x, y, boxWidth, boxHeight);
        }

      }

      function drawLandmarks(detections){
        noFill();
        stroke(161, 95, 251)
        strokeWeight(2)

        for(let i = 0; i < detections.length; i++){
            const mouth = detections[i].parts.mouth;
            const nose = detections[i].parts.nose;
            const leftEye = detections[i].parts.leftEye;
            const rightEye = detections[i].parts.rightEye;
            const rightEyeBrow = detections[i].parts.rightEyeBrow;
            const leftEyeBrow = detections[i].parts.leftEyeBrow;

            drawPart(mouth, true);
            drawPart(nose, false);
            drawPart(leftEye, true);
            drawPart(leftEyeBrow, false);
            drawPart(rightEye, true);
            drawPart(rightEyeBrow, false);

        }

      }

      function drawPart(feature, closed){

        beginShape();
        for(let i = 0; i < feature.length; i++){
            const x = feature[i]._x
            const y = feature[i]._y
            vertex(x, y)
        }

        if(closed === true){
            endShape(CLOSE);
        } else {
            endShape();
        }

      }
    </script>
</body>
</html>
