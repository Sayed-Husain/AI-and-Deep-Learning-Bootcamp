{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Regression with TensorFlow\n"
      ],
      "metadata": {
        "id": "w-a7NcypWql5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction\n",
        "In this notebook, we will delve into the world of neural network regression using TensorFlow. We will build a simple neural network model to perform regression, predicting a continuous output based on input features. We'll go through the steps of creating a synthetic dataset, designing a neural network model, compiling the model, training it, and making predictions. By the end of this notebook, you'll have a basic understanding of how to use TensorFlow for regression tasks."
      ],
      "metadata": {
        "id": "UGuuA3atXiny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. **Creating the Dataset**\n",
        "2. **Building the Neural Network Model**\n",
        "3. **Compiling and Training the Model**\n",
        "4. **Making Predictions**\n",
        "5. **Improve the model**\n",
        "    \n",
        "    5.1. Train for More Epochs\n",
        "\n",
        "    5.2. Increase Model Complexity\n",
        "    \n",
        "    5.3. Use Different Optimizers\n",
        "\n",
        "Let's get started"
      ],
      "metadata": {
        "id": "JId1PJE1Xk9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Creating the Dataset\n",
        "We'll begin by generating a synthetic dataset that follows a linear relationship between features and labels. The features will be stored as TensorFlow tensors for compatibility with the neural network model."
      ],
      "metadata": {
        "id": "b7XmCFXmXxhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create features (using tensors)\n",
        "X = tf.constant([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n",
        "\n",
        "# Create labels (using tensors)\n",
        "y = tf.constant([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n",
        "\n",
        "# Visualize the dataset\n",
        "plt.scatter(X, y)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Labels\")\n",
        "plt.title(\"Synthetic Dataset\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "z5W2QDOqX03O",
        "outputId": "674257fc-b0dd-4b03-e1dc-0344ebf9226d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Building the Neural Network Model\n",
        "Next, we'll design a neural network model using TensorFlow's Sequential API. Our model will consist of a single Dense layer. In this simple example, we're aiming to learn a linear relationship, so a single neuron should be sufficient."
      ],
      "metadata": {
        "id": "itBnfMGjX3px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model using the Sequential API\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(1)  # Single neuron in the layer\n",
        "])"
      ],
      "metadata": {
        "id": "XqpouVrxX7BY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Compiling and Training the Model\n",
        "Once the model is defined, we need to compile it before training. We'll specify the loss function, optimizer, and metrics to monitor during training."
      ],
      "metadata": {
        "id": "4Ne2MnarYAGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss=tf.keras.losses.mae,  # Mean Absolute Error loss\n",
        "              optimizer=tf.keras.optimizers.SGD(),  # Stochastic Gradient Descent optimizer\n",
        "              metrics=[\"mae\"])  # Metric to monitor: Mean Absolute Error\n",
        "\n",
        "# Fit the model to the data\n",
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=5)  # Expand dimensions of X to match input shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9ZVFbbPYCIB",
        "outputId": "592362ae-ce4c-4e11-e8f1-51fd0010bb44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 1s 749ms/step - loss: 12.3340 - mae: 12.3340\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 12.2015 - mae: 12.2015\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 12.0690 - mae: 12.0690\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 11.9365 - mae: 11.9365\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 11.8040 - mae: 11.8040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bcb386161a0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Making Predictions\n",
        "After training, we can use the trained model to make predictions on new data."
      ],
      "metadata": {
        "id": "YSHMiW69YH6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction using the trained model\n",
        "prediction = model.predict([17.0])\n",
        "print(\"Prediction for input 17.0:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCHc_mUMYLnN",
        "outputId": "4fe7d3f9-cde0-44d7-9806-5a739a596d3d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 102ms/step\n",
            "Prediction for input 17.0: [[8.688275]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Improving the Model\n",
        "Now that we have a basic understanding of creating and training a neural network model for regression, let's explore some techniques to improve the model's performance. Keep in mind that model improvement is often an iterative process involving experimentation and analysis."
      ],
      "metadata": {
        "id": "2HJgIT5EYM0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1. Train for More Epochs\n",
        "One straightforward way to potentially improve the model's performance is to train it for more epochs. This allows the model to observe the data for a longer period, which can lead to better convergence and potentially better performance."
      ],
      "metadata": {
        "id": "G5278VmkYVch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model for more epochs\n",
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=100)  # Increase the number of epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX7klg8rYXSP",
        "outputId": "8c3e37a5-9bc3-45df-8e68-f668ad1630a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 11.6715 - mae: 11.6715\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 11.5390 - mae: 11.5390\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 11.4065 - mae: 11.4065\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 11.2740 - mae: 11.2740\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 11.1415 - mae: 11.1415\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 11.0090 - mae: 11.0090\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.8765 - mae: 10.8765\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 10.7440 - mae: 10.7440\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.6115 - mae: 10.6115\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 10.4790 - mae: 10.4790\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.3465 - mae: 10.3465\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 10.2140 - mae: 10.2140\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 10.0815 - mae: 10.0815\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 9.9490 - mae: 9.9490\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.8165 - mae: 9.8165\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.6840 - mae: 9.6840\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.5515 - mae: 9.5515\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.4190 - mae: 9.4190\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 9.2865 - mae: 9.2865\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.1540 - mae: 9.1540\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 9.0215 - mae: 9.0215\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 8.8890 - mae: 8.8890\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.7565 - mae: 8.7565\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.6240 - mae: 8.6240\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 8.4915 - mae: 8.4915\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 8.3590 - mae: 8.3590\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.2265 - mae: 8.2265\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.0940 - mae: 8.0940\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 7.9615 - mae: 7.9615\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.8290 - mae: 7.8290\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.6965 - mae: 7.6965\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.5640 - mae: 7.5640\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.4315 - mae: 7.4315\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.2990 - mae: 7.2990\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 7.2075 - mae: 7.2075\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.2019 - mae: 7.2019\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 7.1962 - mae: 7.1962\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.1906 - mae: 7.1906\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 7.1850 - mae: 7.1850\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1794 - mae: 7.1794\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.1738 - mae: 7.1738\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.1681 - mae: 7.1681\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1625 - mae: 7.1625\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.1569 - mae: 7.1569\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1512 - mae: 7.1512\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1456 - mae: 7.1456\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.1400 - mae: 7.1400\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1344 - mae: 7.1344\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.1287 - mae: 7.1287\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.1231 - mae: 7.1231\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.1175 - mae: 7.1175\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.1119 - mae: 7.1119\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.1063 - mae: 7.1063\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1006 - mae: 7.1006\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.0950 - mae: 7.0950\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.0894 - mae: 7.0894\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.0838 - mae: 7.0838\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.0781 - mae: 7.0781\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.0725 - mae: 7.0725\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.0669 - mae: 7.0669\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.0612 - mae: 7.0612\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.0556 - mae: 7.0556\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.0500 - mae: 7.0500\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.0444 - mae: 7.0444\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.0387 - mae: 7.0387\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.0331 - mae: 7.0331\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0275 - mae: 7.0275\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.0219 - mae: 7.0219\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.0163 - mae: 7.0163\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.0106 - mae: 7.0106\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.0050 - mae: 7.0050\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.9994 - mae: 6.9994\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.9938 - mae: 6.9938\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.9881 - mae: 6.9881\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.9825 - mae: 6.9825\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.9769 - mae: 6.9769\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 6.9713 - mae: 6.9713\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.9656 - mae: 6.9656\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.9600 - mae: 6.9600\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.9544 - mae: 6.9544\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.9488 - mae: 6.9488\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.9431 - mae: 6.9431\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.9375 - mae: 6.9375\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.9319 - mae: 6.9319\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.9263 - mae: 6.9263\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.9206 - mae: 6.9206\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9150 - mae: 6.9150\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9094 - mae: 6.9094\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.9038 - mae: 6.9038\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8981 - mae: 6.8981\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8925 - mae: 6.8925\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8869 - mae: 6.8869\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.8813 - mae: 6.8813\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8756 - mae: 6.8756\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.8700 - mae: 6.8700\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8644 - mae: 6.8644\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.8588 - mae: 6.8588\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.8531 - mae: 6.8531\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.8475 - mae: 6.8475\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.8419 - mae: 6.8419\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bcb382852d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction using the trained model\n",
        "prediction = model.predict([17.0])\n",
        "print(\"Prediction for input 17.0:\", prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b87yMNm7YZ8i",
        "outputId": "ce951aa2-c1fd-4d54-e859-4efc63c492ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 63ms/step\n",
            "Prediction for input 17.0: [[29.753271]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. Increase Model Complexity\n",
        "Increasing the complexity of the model by adding more layers and neurons can help capture more intricate patterns in the data. However, be cautious not to overcomplicate the model, as this might lead to overfitting."
      ],
      "metadata": {
        "id": "-gpp0HwnYbh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a more complex model\n",
        "complex_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),  # Add a hidden layer\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile and fit the complex model\n",
        "complex_model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
        "complex_model.fit(tf.expand_dims(X, axis=-1), y, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGwBUJbtYeM3",
        "outputId": "979ccebc-a293-4bb3-fd5d-acfd874e8b6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 11.9472\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 11.6512\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 11.3536\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 11.0544\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 10.7531\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 10.4496\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 10.1436\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.8351\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.5240\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.2102\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.9342\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.6848\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.4328\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 8.1782\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.9213\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.6624\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.4006\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1359\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8694\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6015\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.3363\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.0699\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.7994\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.5245\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.2454\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.9630\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.6764\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3852\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.0886\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.9068\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8091\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.7147\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6836\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.7283\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.7664\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.7996\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.8273\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.8492\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.8648\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.8759\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.8830\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.8863\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.8862\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.8830\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8769\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 3.8682\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8566\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8426\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8266\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8086\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.7885\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.7662\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.7421\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.7165\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.6895\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.6624\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6344\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6056\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.5759\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.5441\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.5184\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.4860\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 3.4478\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.4146\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.3774\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.3410\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.3102\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3242\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3286\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.3236\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.3098\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.2879\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.2585\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.2251\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.2131\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.2106\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.2039\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.1934\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.1792\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.1616\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.1404\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.1158\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.0884\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.0585\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0460\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.0365\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.0157\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.9845\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.9532\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.9464\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 2.9186\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8956\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.8693\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.8424\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.8188\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.7948\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.7708\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 2.7430\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7105\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.6830\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bcb3822bd00>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction using the trained model\n",
        "prediction = model.predict([17.0])\n",
        "print(\"Prediction for input 17.0:\", prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHH51xmGYisg",
        "outputId": "12448d24-ee86-44e9-d73b-690f8ebb9e6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction for input 17.0: [[29.753271]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Use Different Optimizers\n",
        "The choice of optimizer can significantly impact the training process. Experiment with different optimizers, such as Adam or RMSprop, to find the one that works best for your dataset and model architecture."
      ],
      "metadata": {
        "id": "3-3kUDmaYlnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try using a different optimizer\n",
        "model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "model.fit(tf.expand_dims(X, axis=-1), y, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcgTTpF1YpS9",
        "outputId": "c5af390d-2fc2-46b1-de10-28f68b109660"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 427ms/step - loss: 6.8363\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 6.8355\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.8348\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.8340\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.8333\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8325\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.8317\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8310\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8303\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8295\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8288\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.8280\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8273\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8265\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8258\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8250\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8243\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8235\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8228\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8220\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8213\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8205\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8198\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8190\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8183\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.8175\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.8168\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8160\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8153\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8145\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8138\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.8130\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.8123\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8115\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8108\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8100\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.8093\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8085\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8078\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8070\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8063\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8055\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.8048\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.8040\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.8033\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.8025\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8018\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.8010\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.8003\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.7995\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.7988\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.7980\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7973\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 6.7965\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7958\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7950\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7943\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.7935\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7928\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7920\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7913\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7905\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7898\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7890\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7883\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7875\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7868\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7860\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7853\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7845\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7838\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7830\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7823\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.7815\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.7808\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7800\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7793\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.7785\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.7778\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.7770\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.7763\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7755\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7748\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7740\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7733\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7725\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.7718\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.7710\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7703\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7695\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7688\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7680\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7673\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.7665\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.7658\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7650\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7643\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7635\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.7628\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7620\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7bcb3817b580>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction using the trained model\n",
        "prediction = model.predict([17.0])\n",
        "print(\"Prediction for input 17.0:\", prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1eXwstYYrri",
        "outputId": "d78adb84-4a16-4599-94e0-28fef9c081f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 56ms/step\n",
            "Prediction for input 17.0: [[29.85327]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "Remember that deep learning is an iterative process that involves experimentation, tuning, and analysis. This notebook provides a foundation for your future explorations in more complex deep learning tasks. By experimenting with different architectures, loss functions, and optimization strategies, you can refine your models to achieve better results.\n",
        "\n",
        "Keep up the great work and happy learning!"
      ],
      "metadata": {
        "id": "MjU7zz-8Ys4T"
      }
    }
  ]
}